{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Connect to gdrive\n",
    "import google.colab.drive\n",
    "google.colab.drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Data loading utility\n",
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import json\n",
    "import networkx as nx\n",
    "\n",
    "class DatasetLoader:\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "    def extract_dataset(self, file_name):\n",
    "        full_path = os.path.join(self.data_dir, file_name)\n",
    "        with tarfile.open(full_path) as tar:\n",
    "            tar.extractall(path=self.data_dir)\n",
    "        print(f\"Dataset extracted to {self.data_dir}\")\n",
    "        return self.data_dir\n",
    "\n",
    "    def load_data(self, locale, subfolder = ''):  # 'HR' for Croatia, 'HU' for Hungary, 'RO' for Romania\n",
    "        edges_file = os.path.join(self.data_dir, subfolder, locale + '_edges.csv')\n",
    "        genres_file = os.path.join(self.data_dir, subfolder, locale + '_genres.json')\n",
    "\n",
    "        edges = pd.read_csv(edges_file)\n",
    "        with open(genres_file, 'r') as f:\n",
    "            genres = json.load(f)\n",
    "        return edges, genres\n",
    "\n",
    "# Constants\n",
    "DATASET_ARCHIVE = \"gemsec_deezer_dataset.tar.gz\"\n",
    "EXTRACT_DIR = \"/content/drive/MyDrive/network_science/group_project_deezer/\"\n",
    "\n",
    "dataset_loader = DatasetLoader(EXTRACT_DIR)\n",
    "dataset_loader.extract_dataset(DATASET_ARCHIVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data for RO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title load data for RO\n",
    "LOCALE = \"RO\"\n",
    "\n",
    "edges, genres = dataset_loader.load_data(LOCALE, subfolder='deezer_clean_data')\n",
    "G_RO = nx.from_pandas_edgelist(edges, source='node_1', target='node_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title find communities using louvain\n",
    "from networkx.algorithms import community\n",
    "communities = list(community.louvain_communities(G_RO, seed=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Precompute graph node details and save them to CSV\n",
    "import csv\n",
    "\n",
    "community_data = {}\n",
    "for i, community in enumerate(communities):\n",
    "    subgraph = G_RO.subgraph(community)\n",
    "\n",
    "    degree_centrality = nx.degree_centrality(subgraph)\n",
    "    closeness_centrality = nx.closeness_centrality(subgraph)\n",
    "    betweenness_centrality = nx.betweenness_centrality(subgraph)\n",
    "\n",
    "    for node in community:\n",
    "        if str(node) in genres:  # Ensure genres are available\n",
    "            node_data = {\n",
    "                \"node_id\": node,\n",
    "                \"community_id\": i,\n",
    "                \"community_size\": len(community),\n",
    "                \"betweenness_centrality\": betweenness_centrality.get(node, None),\n",
    "                \"degree_centrality\": degree_centrality.get(node, None),\n",
    "                \"closeness_centrality\": closeness_centrality.get(node, None),\n",
    "                \"genres\": genres[str(node)]\n",
    "            }\n",
    "            community_data[node] = node_data\n",
    "        else:\n",
    "            print(f\"Genres not found for node {node}\")\n",
    "\n",
    "output_csv_file = \"/content/drive/MyDrive/network_science/group_project_deezer/node_data_RO_v2.csv\"\n",
    "with open(output_csv_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames={\n",
    "                                                  \"node_id\",\n",
    "                                                  \"community_id\",\n",
    "                                                  \"community_size\",\n",
    "                                                  \"betweenness_centrality\",\n",
    "                                                  \"degree_centrality\",\n",
    "                                                  \"closeness_centrality\",\n",
    "                                                  \"genres\"\n",
    "                                              })\n",
    "    writer.writeheader()\n",
    "    for node, node_data in community_data.items():\n",
    "      writer.writerow(node_data)\n",
    "\n",
    "print(f\"Node data saved to {output_csv_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data for HR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title load data for HR\n",
    "LOCALE = \"HR\"\n",
    "edges, genres = dataset_loader.load_data(LOCALE, subfolder='deezer_clean_data')\n",
    "G_HR = nx.from_pandas_edgelist(edges, source='node_1', target='node_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title find communities using louvain\n",
    "from networkx.algorithms import community\n",
    "communities = list(community.louvain_communities(G_HR, seed=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Precompute graph node details and save them to CSV\n",
    "import csv\n",
    "\n",
    "community_data = {}\n",
    "for i, community in enumerate(communities):\n",
    "    subgraph = G_HR.subgraph(community)\n",
    "\n",
    "    degree_centrality = nx.degree_centrality(subgraph)\n",
    "    closeness_centrality = nx.closeness_centrality(subgraph)\n",
    "    betweenness_centrality = nx.betweenness_centrality(subgraph)\n",
    "\n",
    "    for node in community:\n",
    "        if str(node) in genres:\n",
    "            node_data = {\n",
    "                \"node_id\": node,\n",
    "                \"community_id\": i,\n",
    "                \"community_size\": len(community),\n",
    "                \"betweenness_centrality\": betweenness_centrality.get(node, None),\n",
    "                \"degree_centrality\": degree_centrality.get(node, None),\n",
    "                \"closeness_centrality\": closeness_centrality.get(node, None),\n",
    "                \"genres\": genres[str(node)]\n",
    "            }\n",
    "            community_data[node] = node_data\n",
    "        else:\n",
    "            print(f\"Genres not found for node {node}\")\n",
    "\n",
    "output_csv_file = \"/content/drive/MyDrive/network_science/group_project_deezer/node_data_HR_v2.csv\"\n",
    "with open(output_csv_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames={\n",
    "                                                  \"node_id\",\n",
    "                                                  \"community_id\",\n",
    "                                                  \"community_size\",\n",
    "                                                  \"betweenness_centrality\",\n",
    "                                                  \"degree_centrality\",\n",
    "                                                  \"closeness_centrality\",\n",
    "                                                  \"genres\"\n",
    "                                              })\n",
    "    writer.writeheader()\n",
    "    for node, node_data in community_data.items():\n",
    "      writer.writerow(node_data)\n",
    "\n",
    "print(f\"Node data saved to {output_csv_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data for HU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title load data for HU\n",
    "LOCALE = \"HU\"\n",
    "edges, genres = dataset_loader.load_data(LOCALE, subfolder='deezer_clean_data')\n",
    "G_HU = nx.from_pandas_edgelist(edges, source='node_1', target='node_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title find communities using louvain\n",
    "from networkx.algorithms import community\n",
    "communities = list(community.louvain_communities(G_HU, seed=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Precompute graph node details and save them to CSV\n",
    "import csv\n",
    "\n",
    "community_data = {}\n",
    "for i, community in enumerate(communities):\n",
    "    subgraph = G_HU.subgraph(community)\n",
    "\n",
    "    degree_centrality = nx.degree_centrality(subgraph)\n",
    "    closeness_centrality = nx.closeness_centrality(subgraph)\n",
    "    betweenness_centrality = nx.betweenness_centrality(subgraph)\n",
    "\n",
    "    for node in community:\n",
    "        if str(node) in genres:\n",
    "            node_data = {\n",
    "                \"node_id\": node,\n",
    "                \"community_id\": i,\n",
    "                \"community_size\": len(community),\n",
    "                \"betweenness_centrality\": betweenness_centrality.get(node, None),\n",
    "                \"degree_centrality\": degree_centrality.get(node, None),\n",
    "                \"closeness_centrality\": closeness_centrality.get(node, None),\n",
    "                \"genres\": genres[str(node)]\n",
    "            }\n",
    "            community_data[node] = node_data\n",
    "        else:\n",
    "            print(f\"Genres not found for node {node}\")\n",
    "\n",
    "output_csv_file = \"/content/drive/MyDrive/network_science/group_project_deezer/node_data_HU_v2.csv\"\n",
    "with open(output_csv_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames={\n",
    "                                                  \"node_id\",\n",
    "                                                  \"community_id\",\n",
    "                                                  \"community_size\",\n",
    "                                                  \"betweenness_centrality\",\n",
    "                                                  \"degree_centrality\",\n",
    "                                                  \"closeness_centrality\",\n",
    "                                                  \"genres\"\n",
    "                                              })\n",
    "    writer.writeheader()\n",
    "    for node, node_data in community_data.items():\n",
    "      writer.writerow(node_data)\n",
    "\n",
    "print(f\"Node data saved to {output_csv_file}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
